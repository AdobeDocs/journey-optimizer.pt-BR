---
product: experience platform
solution: Experience Platform
title: Modelos de otimiza√ß√£o autom√°tica
description: Saiba mais sobre os modelos de otimiza√ß√£o autom√°tica
feature: Ranking, Decision Management
role: User
level: Experienced
source-git-commit: 58f4fdf8ec3cdb609efebf5b8713f6b770ef5414
workflow-type: tm+mt
source-wordcount: '1348'
ht-degree: 0%

---

# Modelos de otimiza√ß√£o autom√°tica {#auto-optimization-model}

Um modelo de otimiza√ß√£o autom√°tica visa fornecer ofertas que maximizam o retorno (KPIs) definido pelos clientes empresariais. Esses KPIs podem estar na forma de taxas de convers√£o, receita etc. Nesse ponto, a Otimiza√ß√£o autom√°tica se concentra na otimiza√ß√£o de cliques na oferta com a convers√£o de oferta como destino. A otimiza√ß√£o autom√°tica n√£o √© personalizada e √© otimizada com base no desempenho &quot;global&quot; das ofertas.

## Limita√ß√µes {#limitations}

O uso de modelos de otimiza√ß√£o autom√°tica para a gest√£o de decis√µes est√° sujeito √†s limita√ß√µes abaixo:

<!--* Auto-optimization models do not work with the Batch Decisioning API.-->
* O feedback necess√°rio para criar o modelo deve ser enviado como um evento de experi√™ncia. Ele n√£o deve ser enviado automaticamente em [!DNL Journey Optimizer] canais.

## Terminologia {#terminology}

Os termos a seguir s√£o √∫teis ao discutir a Otimiza√ß√£o autom√°tica:

* **Bandit multi-armed**: uma abordagem [bandit multi-armed](https://en.wikipedia.org/wiki/Multi-armed_bandit){target="_blank"} √† otimiza√ß√£o equilibra o aprendizado explorat√≥rio e o aproveitamento desse aprendizado.

* **Amostragem de Thomson**: a amostragem de Thompson √© um algoritmo para problemas de decis√£o online em que as a√ß√µes s√£o tomadas sequencialmente de uma maneira que deve equilibrar entre explorar o que √© conhecido por maximizar o desempenho imediato e investir para acumular novas informa√ß√µes que podem melhorar o desempenho futuro. [Saiba mais](#thompson-sampling)

* [**Distribui√ß√£o de Beta**](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}: conjunto de [distribui√ß√µes de probabilidade](https://en.wikipedia.org/wiki/Probability_distribution){target="_blank"} cont√≠nuas definidas no intervalo [0, 1] [parametrizadas](https://en.wikipedia.org/wiki/Statistical_parameter){target="_blank"} por dois [par√¢metros de forma](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"} positivos.

## Amostragem de Thompson {#thompson-sampling}

O algoritmo subjacente √† Otimiza√ß√£o autom√°tica √© **Amostragem de Thompson**. Nesta se√ß√£o, discutimos a intui√ß√£o por tr√°s da Amostragem de Thompson.

[Amostragem de Thompson](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, ou bandidos Bayesianos, √© uma abordagem Bayesiana para o problema do bandido multi-armado.  A ideia b√°sica √© tratar a m√©dia da recompensa ùõç de cada oferta como uma **vari√°vel aleat√≥ria** e usar os dados que coletamos at√© agora para atualizar nossa &quot;cren√ßa&quot; sobre a m√©dia da recompensa. Essa &quot;cren√ßa&quot; √© representada matematicamente por uma **distribui√ß√£o de probabilidade posterior** - essencialmente um intervalo de valores para a recompensa m√©dia, juntamente com a plausibilidade (ou probabilidade) de que a recompensa tenha esse valor para cada oferta.‚ÄØEm seguida, para cada decis√£o, **faremos uma amostra de um ponto de cada uma dessas distribui√ß√µes de recompensa posteriores** e selecionaremos a oferta cuja recompensa de amostra teve o valor mais alto.

Esse processo √© ilustrado na figura abaixo, onde temos 3 ofertas diferentes. Inicialmente, n√£o temos nenhuma evid√™ncia dos dados e assumimos que todas as ofertas t√™m uma distribui√ß√£o de recompensa posterior uniforme. Tiramos uma amostra da distribui√ß√£o de recompensa posterior de cada oferta. A amostra selecionada na distribui√ß√£o da Oferta 2 tem o valor mais alto. Este √© um exemplo de **explora√ß√£o**. Depois de mostrar a Oferta 2, coletamos qualquer recompensa potencial (por exemplo, convers√£o/sem convers√£o) e atualizamos a distribui√ß√£o posterior da Oferta 2 usando o Teorema de Bayes como explicado abaixo.  Continuamos esse processo e atualizamos as distribui√ß√µes posteriores sempre que uma oferta √© exibida e a recompensa √© coletada. Na segunda figura, a Oferta 3 √© selecionada - apesar de a Oferta 1 ter a maior recompensa m√©dia (sua distribui√ß√£o de recompensa posterior √© a mais √† direita), o processo de amostragem de cada distribui√ß√£o levou-nos a escolher uma Oferta 3 aparentemente abaixo do ideal. Ao fazer isso, oferecemos a n√≥s mesmos a oportunidade de aprender mais sobre a verdadeira distribui√ß√£o de recompensas da Oferta 3.

√Ä medida que mais amostras s√£o coletadas, a confian√ßa aumenta e uma estimativa mais precisa da poss√≠vel recompensa √© obtida (correspondendo a distribui√ß√µes de recompensa mais estreitas).‚ÄØEsse processo de atualiza√ß√£o de nossas cren√ßas √† medida que mais evid√™ncias se tornam dispon√≠veis √© conhecido como **Infer√™ncia Bayesiana**.

Eventualmente, se uma oferta (por exemplo, Oferta 1) for um vencedor claro, sua distribui√ß√£o de recompensa posterior ser√° separada das outras. Neste ponto, para cada decis√£o, a recompensa amostrada da Oferta 1 provavelmente ser√° a mais alta, e n√≥s a escolheremos com uma probabilidade mais alta. Isto √© **explora√ß√£o** - acreditamos que a Oferta 1 √© a melhor e, portanto, est√° sendo escolhida para maximizar as recompensas.

![](../assets/ai-ranking-thompson-sampling.png)

**Figura 1**: *Para cada decis√£o, analisamos um ponto das distribui√ß√µes posteriores da recompensa. A oferta com o valor de amostra mais alto (taxa de convers√£o) ser√° escolhida. Na fase inicial, todas as ofertas t√™m distribui√ß√£o uniforme, j√° que n√£o temos nenhuma evid√™ncia sobre as taxas de convers√£o das ofertas com base nos dados. √Ä medida que coletamos mais amostras, as distribui√ß√µes posteriores ficam mais estreitas e precisas. Por fim, a oferta com a maior taxa de convers√£o ser√° escolhida sempre.*

+++**Detalhes t√©cnicos**

Para calcular/atualizar distribui√ß√µes, usamos o **Teorema de Bayes**. Para cada oferta ***i***, queremos calcular seus ***P(ùõçi | data)***, ou seja, para cada oferta ***i***, a probabilidade de um valor de recompensa **ùõçi** ser, dados os dados que coletamos at√© agora para essa oferta.

A partir do Teorema de Bayes:

***Posterior = Probabilidade * Anterior***

A **probabilidade anterior** √© a estimativa inicial sobre a probabilidade de produzir uma sa√≠da. A probabilidade, ap√≥s a coleta de algumas evid√™ncias, √© conhecida como **probabilidade posterior**.‚ÄØ

A otimiza√ß√£o autom√°tica foi projetada para considerar recompensas bin√°rias (clique/sem clique). Neste caso, a probabilidade representa o n√∫mero de sucessos de N tentativas e √© modelada por uma **distribui√ß√£o binomial**. Para algumas fun√ß√µes veross√≠meis, se voc√™ escolher um determinado anterior, o posterior acaba ficando na mesma distribui√ß√£o que o anterior. O anterior ent√£o √© chamado de **conjugado anterior**. Esse tipo de an√°lise pr√©via torna muito simples o c√°lculo da distribui√ß√£o posterior. A **distribui√ß√£o de Beta** √© um conjugado antes da probabilidade binomial (recompensas bin√°rias), por isso √© uma escolha conveniente e sensata para as distribui√ß√µes de probabilidade anterior e posterior. A distribui√ß√£o de Beta usa dois par√¢metros, ***Œ±*** e ***Œ≤***.‚ÄØEsses par√¢metros podem ser considerados como a contagem de sucessos e falhas e o valor m√©dio fornecido por:

![](../assets/ai-ranking-beta-distribution.png)

A fun√ß√£o Probabilidade, como explicamos acima, √© modelada por uma distribui√ß√£o Binomial, com s sucessos (convers√µes) e f falhas (sem convers√µes) e q √© uma [vari√°vel aleat√≥ria](https://en.wikipedia.org/wiki/Random_variable){target="_blank"} com uma [distribui√ß√£o beta](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}.

O anterior √© modelado pela distribui√ß√£o do Beta e a distribui√ß√£o posterior assume a seguinte forma:

![](../assets/ai-ranking-posterior-distribution.svg)

A posterior √© calculada simplesmente adicionando o n√∫mero de sucessos e falhas aos par√¢metros existentes ***Œ±***, ***Œ≤***.

Para otimiza√ß√£o autom√°tica, como mostrado no exemplo acima, come√ßamos com uma distribui√ß√£o anterior ***Beta(1, 1)*** (distribui√ß√£o uniforme) para todas as ofertas e, depois de obter s sucessos e f falhas para uma determinada oferta, a parte posterior se torna uma distribui√ß√£o do Beta com par√¢metros ***(s+Œ±, f+Œ≤)*** para essa oferta.
+++

**T√≥picos relacionados**:

Para aprofundar a amostragem de Thompson, leia os seguintes artigos de pesquisa:

* [Uma Avalia√ß√£o Emp√≠rica da Amostragem de Thompson](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [An√°lise de Amostragem de Thompson para o problema do Multi-armed Bandit](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## Problema de arranque a frio {#cold-start}

O problema de &quot;inicializa√ß√£o imediata&quot; ocorre quando uma nova oferta √© adicionada a uma campanha e n√£o h√° dados dispon√≠veis sobre a taxa de convers√£o da nova oferta. Durante esse per√≠odo, temos que criar uma estrat√©gia com rela√ß√£o √† frequ√™ncia com que essa nova oferta √© escolhida para que a queda de desempenho seja minimizada, enquanto coletamos informa√ß√µes sobre a taxa de convers√£o dessa nova oferta. H√° v√°rias solu√ß√µes dispon√≠veis para resolver esse problema. A chave √© encontrar um equil√≠brio entre a explora√ß√£o dessa nova oferta, enquanto n√£o sacrificamos muito a explora√ß√£o. Atualmente, usamos &quot;distribui√ß√£o uniforme&quot; como nossa suposi√ß√£o inicial sobre a taxa de convers√£o da nova oferta (distribui√ß√£o anterior). Basicamente, damos a todos os valores de taxa de convers√£o probabilidade igual de ocorr√™ncia.

![](../assets/ai-ranking-cold-start-strategies.png)

**Figura 2**: *Considere uma campanha com 3 ofertas. Enquanto a campanha est√° ativa, a Oferta 4 √© adicionada √† campanha. Inicialmente, n√£o temos dados sobre a taxa de convers√£o da Oferta 4 e temos que lidar com o problema de inicializa√ß√£o a frio. Usamos a distribui√ß√£o uniforme como nossa estimativa inicial sobre a taxa de convers√£o da Oferta 4, enquanto coletamos dados para essa nova oferta. Conforme explicado na se√ß√£o [Amostragem de Thompson](#thompson-sampling), para escolher qual oferta ser√° exibida a um usu√°rio, fazemos a amostragem de pontos das distribui√ß√µes posteriores de recompensas das ofertas e selecionamos a oferta com o maior valor de amostra. No exemplo acima, a Oferta 4 √© escolhida e, posteriormente, com base na recompensa coletada. A distribui√ß√£o posterior dessa oferta √© atualizada conforme explicado na se√ß√£o [Amostragem de Thompson](#thompson-sampling).*

## Medi√ß√£o de aumento {#lift}

&quot;Aumento&quot; √© a m√©trica usada para medir o desempenho de qualquer estrat√©gia implantada no servi√ßo de classifica√ß√£o, em compara√ß√£o com a estrat√©gia da linha de base (apresentando ofertas apenas aleatoriamente).

Por exemplo, se estivermos interessados em medir o desempenho de uma estrat√©gia de Thompson Sampling (TS) usada no servi√ßo de classifica√ß√£o, e o KPI for taxa de convers√£o (CVR), o &quot;aumento&quot; da estrat√©gia de TS em rela√ß√£o √† estrat√©gia de linha de base √© definido como:

![](../assets/ai-ranking-lift.png)
